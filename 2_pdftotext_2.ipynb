{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf # imports the pymupdf library\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# pip install clean-text\n",
    "# pip install unidecode\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from pdfs, returns the text from the passed file\n",
    "def PDFtoText(filename):\n",
    "  doc = pymupdf.open(filename) # open a document\n",
    "  for page in doc: # iterate the document pages\n",
    "    text = page.get_text() # get plain text encoded as UTF-8\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single file demo\n",
    "# print(PDFtoText(\"combinedfiles/000e3869f09a1f4e92d466218fa2de17-cv.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = clean(text,\n",
    "                         fix_unicode=True,  # fix various unicode errors\n",
    "                         to_ascii=False,     # transliterate to closest ASCII representation\n",
    "                         lower=False,       # lowercase text\n",
    "                         no_line_breaks=True,  # remove line breaks\n",
    "                         no_urls=True,      # replace all URLs with a special token\n",
    "                         no_emails=True,    # replace all email addresses with a special token\n",
    "                         no_phone_numbers=True,  # replace all phone numbers with a special token\n",
    "                         no_numbers=False,  # remove all numbers\n",
    "                         no_digits=False,   # remove all digits\n",
    "                         no_currency_symbols=True,  # remove all currency symbols\n",
    "                         no_punct=True,     # remove punctuations\n",
    "                         replace_with_url=\"<URL>\",\n",
    "                         replace_with_email=\"<EMAIL>\",\n",
    "                         replace_with_phone_number=\"<PHONE_NUMBER>\",\n",
    "                         replace_with_number=\"<NUMBER>\",\n",
    "                         replace_with_digit=\"0\",\n",
    "                         replace_with_currency_symbol=\"<CUR>\",\n",
    "                         lang=\"en\"          # set to 'en' for English\n",
    "                         )\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id (filename):\n",
    "    return filename[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get filelist from folder\n",
    "# pdffiles = os.listdir(\"combinedfiles\")\n",
    "\n",
    "# directory_path = \"combinedfiles\"\n",
    "\n",
    "# extracted_texts = [] # list to store the texts\n",
    "# for file in tqdm(pdffiles, desc=\"Progress\"):\n",
    "#     file_path = os.path.join(directory_path, file)\n",
    "#     text_result = PDFtoText(file_path)\n",
    "#     # cleaned_text = text_result.replace('\\n', ' ')\n",
    "#     cleaned_text = clean_text(text_result) # this function is called here, and again below; seems that calling it multiple times improves the results.\n",
    "#     # cleaned_text = text_result\n",
    "#     ID_Nominations = extract_id(file)\n",
    "#     extracted_texts.append((ID_Nominations, file, cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(extracted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Convert the list to a pandas DataFrame\n",
    "# df_extracted_texts = pd.DataFrame(extracted_texts, columns=['ID_Nominations', 'Filename', 'Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame to a CSV file with specified options\n",
    "# # df_extracted_texts.to_csv(\"extracted_texts_utf16.csv\", index=False, quoting=1, encoding='utf-16')\n",
    "# df_extracted_texts.to_csv(\"texts_extracted_raw_utf8.csv\", index=False, quoting=1, encoding='utf-8')\n",
    "\n",
    "# df_extracted_texts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_texts = pd.read_csv(\"texts_extracted_raw_utf8.csv\") # if rerun and you want to load the text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is unneccessary if we apply ScrubaDub below.\n",
    "\n",
    "# # Apply cleaning to the results column\n",
    "\n",
    "df_cleaned_texts = df_extracted_texts \n",
    "\n",
    "df_cleaned_texts['Result'] = df_cleaned_texts['Result'].apply(clean_text)\n",
    "df_cleaned_texts['Result'] = df_cleaned_texts['Result'].apply(clean_text) # Doppelt h√§lt besser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame to a CSV file with specified options\n",
    "# # df_cleaned_texts.to_csv(\"cleaned_texts_utf16.csv\", index=False, quoting=1, encoding='utf-16')\n",
    "df_cleaned_texts.to_csv(\"texts_cleaned_utf8.csv\", index=False, quoting=1, encoding='utf-8')\n",
    "\n",
    "df_cleaned_texts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also remove stopwords and do stemming...\n",
    "https://github.com/prasanthg3/cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Names with Pseudo-Names in array\n",
    "\n",
    "### # This code is unneccessary if we apply ScrubaDub below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load a list that contains old (real) names and new (fake) names.\n",
    "\n",
    "namereplacements = np.genfromtxt(\"name-replacement_table.csv\", delimiter=\";\", dtype=str)\n",
    "namereplacements.dtype\n",
    "print(namereplacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the list into a nice pd dataframe\n",
    "\n",
    "df_namereplacements = pd.DataFrame(namereplacements)\n",
    "df_namereplacements.columns = [\"ID_Nominations\", \"ID_Persons\", \"new_namefirst\", \"new_namelast\", \"old_namefirst_1\", \"old_namelast_1\", \"old_namefirst_2\", \"old_namelast_2\"]\n",
    "df_namereplacements = df_namereplacements.drop(index=0) # drop header line\n",
    "df_namereplacements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts_to_be_anonymized = pd.read_csv(\"texts_cleaned_utf8.csv\")\n",
    "\n",
    "# merge the text df with the name replacements DF, so that rows are matched correctly.\n",
    "df_merged_texts = df_texts_to_be_anonymized.merge(df_namereplacements, on='ID_Nominations', how='left')\n",
    "df_merged_texts.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces the old_namefirst_2, which is the Pblic Directory name from the Persons table\n",
    "# relevant for different ways of writing, and in case of name changes\n",
    "# it is performed first, because the name_2 is more current than the name_1\n",
    "\n",
    "\n",
    "\n",
    "# replaces the old_namefirst_2, which is the Public Directory name from the Persons table\n",
    "def replace_names1(row):\n",
    "    # if the name field is not empty\n",
    "    if not (pd.isna(row['old_namefirst_2']) or row['old_namefirst_2'] == \"\"):\n",
    "        result_text = str(row['Result']) if not pd.isna(row['Result']) and row['Result'] != \"\" else \"\"\n",
    "        name_old = str(row['old_namefirst_2']) if not pd.isna(row['old_namefirst_2']) and row['old_namefirst_2'] != \"\" else \"\"\n",
    "        name_new = str(row['new_namefirst']) if not pd.isna(row['new_namefirst']) and row['new_namefirst'] != \"\" else \"\"\n",
    "        \n",
    "        if name_old in result_text:\n",
    "            return result_text.replace(name_old, name_new)\n",
    "        \n",
    "    return row['Result']\n",
    "\n",
    "# replaces the old_namefirst_1, which is the main name from the Persons table\n",
    "def replace_names2(row):\n",
    "    # if the name field is not empty\n",
    "    if not (pd.isna(row['old_namefirst_1']) or row['old_namefirst_1'] == \"\"):\n",
    "        result_text = str(row['Result']) if not pd.isna(row['Result']) and row['Result'] != \"\" else \"\"\n",
    "        name_old = str(row['old_namefirst_1']) if not pd.isna(row['old_namefirst_1']) and row['old_namefirst_1'] != \"\" else \"\"\n",
    "        name_new = str(row['new_namefirst']) if not pd.isna(row['new_namefirst']) and row['new_namefirst'] != \"\" else \"\"\n",
    "        \n",
    "        if name_old in result_text:\n",
    "            return result_text.replace(name_old, name_new)\n",
    "        \n",
    "    return row['Result']\n",
    "\n",
    "def replace_names3(row):\n",
    "    # if the name field is not empty\n",
    "    if not (pd.isna(row['old_namelast_2']) or row['old_namelast_2'] == \"\"):\n",
    "        result_text = str(row['Result']) if not pd.isna(row['Result']) and row['Result'] != \"\" else \"\"\n",
    "        name_old = str(row['old_namelast_2']) if not pd.isna(row['old_namelast_2']) and row['old_namelast_2'] != \"\" else \"\"\n",
    "        name_new = str(row['new_namelast']) if not pd.isna(row['new_namelast']) and row['new_namelast'] != \"\" else \"\"\n",
    "        \n",
    "        if name_old in result_text:\n",
    "            return result_text.replace(name_old, name_new)\n",
    "        \n",
    "    return row['Result']\n",
    "\n",
    "def replace_names4(row):\n",
    "    # if the name field is not empty\n",
    "    if not (pd.isna(row['old_namelast_1']) or row['old_namelast_1'] == \"\"):\n",
    "        result_text = str(row['Result']) if not pd.isna(row['Result']) and row['Result'] != \"\" else \"\"\n",
    "        name_old = str(row['old_namelast_1']) if not pd.isna(row['old_namelast_1']) and row['old_namelast_1'] != \"\" else \"\"\n",
    "        name_new = str(row['new_namelast']) if not pd.isna(row['new_namelast']) and row['new_namelast'] != \"\" else \"\"\n",
    "        \n",
    "        if name_old in result_text:\n",
    "            return result_text.replace(name_old, name_new)\n",
    "        \n",
    "    return row['Result']\n",
    "\n",
    "# Apply the function to each row\n",
    "df_merged_texts['Result'] = df_merged_texts.apply(replace_names1, axis=1)\n",
    "df_merged_texts['Result'] = df_merged_texts.apply(replace_names2, axis=1)\n",
    "df_merged_texts['Result'] = df_merged_texts.apply(replace_names3, axis=1)\n",
    "df_merged_texts['Result'] = df_merged_texts.apply(replace_names4, axis=1)\n",
    "\n",
    "df_merged_texts.to_csv(\"texts_anonymized_utf8.csv\", index=False, quoting=1, encoding='utf-8')\n",
    "\n",
    "df_merged_texts.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "\n",
    "df_final_texts = df_merged_texts.drop([\"new_namefirst\", \"new_namelast\", \"old_namefirst_1\", \"old_namelast_1\", \"old_namefirst_2\", \"old_namelast_2\", \"ID_Persons\"], axis=1)\n",
    "\n",
    "df_final_texts.to_csv(\"texts_final_without_scrubbing_utf8.csv\", index=False, quoting=1, encoding='utf-8')\n",
    "\n",
    "df_final_texts.head()\n",
    "\n",
    "# df_final_texts = df_cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymization via NER using Scrubadub / Spacy\n",
    "Removes all names, organizations, PII  - too much, want to keep names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrubadub, scrubadub_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.load('en_core_web_trf')\n",
    "\n",
    "scrubber = scrubadub.Scrubber()\n",
    "scrubber.add_detector(scrubadub_spacy.detectors.SpacyEntityDetector)\n",
    "print(scrubber.clean(\"My name is Alex, I work at LifeGuard in London, and my eMail is alex@lifeguard.com btw. my super secret twitter login is username: alex_2000 password: g-dragon180888\"))\n",
    "# My name is {{NAME}}, I work at {{ORGANIZATION}} in {{LOCATION}}, and my eMail is {{EMAIL}} btw. my super secret twitter login is username: {{USERNAME}} password: {{PASSWORD}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_filename(filename):\n",
    "    return filename[33:-4]\n",
    "\n",
    "# Apply the function to the 'Filename' column\n",
    "df_final_texts['Type'] = df_final_texts['Filename'].apply(clean_filename)\n",
    "\n",
    "# Save the DataFrame to a CSV file with specified options\n",
    "df_final_texts.to_csv('final_texts_non-scrubbed_full.csv', index=False, quoting=1, encoding='utf-8')\n",
    "\n",
    "df_final_texts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kylemclaren/scrub\n",
    "https://microsoft.github.io/presidio/analyzer/\n",
    "AWS comprehend\n",
    "pytesseract for ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Only do it for a smller subset of the data, as it takes long...\n",
    "# approx 4 hours\n",
    "# df_scrub = df_final_texts.iloc[:10, :]\n",
    "df_scrub = df_final_texts\n",
    "\n",
    "\n",
    "ProcessedTexts = []\n",
    "i = 0\n",
    "\n",
    "# Loop over the existing DataFrame rows with a progress bar\n",
    "for index, row in tqdm(df_scrub.iterrows(), desc=\"Processing text\", total=df_scrub.shape[0]):\n",
    "    ID_Nominations = row['ID_Nominations']\n",
    "    Filename = row['Filename']\n",
    "    Result = row['Result']\n",
    "    Type = row['Type']\n",
    "    # ID_Persons = row['ID_Persons']\n",
    "    \n",
    "    processed_result = scrubber.clean(Result)\n",
    "    #ProcessedTexts.append((ID_Nominations, Type, Filename, processed_result))\n",
    "    ProcessedTexts.append((ID_Nominations, Type, processed_result))\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "df_texts_scrubbed = pd.DataFrame(ProcessedTexts, columns=['ID_Nominations', 'Filename', 'Result'])\n",
    "\n",
    "# Save the DataFrame to a CSV file with specified options\n",
    "df_texts_scrubbed.to_csv('final_texts_scrubbed_full.csv', index=False, quoting=1, encoding='utf-8')\n",
    "\n",
    "df_texts_scrubbed.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataVisualization_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
