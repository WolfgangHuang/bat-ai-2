{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip setuptools wheel\n",
    "# %pip install word2vec\n",
    "# %pip install gensim\n",
    "# %pip install tfidf\n",
    "# %pip install cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "import csv\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv file\n",
    "with open('agg-cv-v2.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    text_corpus = list(reader)\n",
    "\n",
    "text_corpus_p0 = [row for row in text_corpus if any(field.strip() for field in row)]\n",
    "\n",
    "for row in text_corpus_p0:\n",
    "    print(\", \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_corpus_p0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define empty list\n",
    "text_corpus_p1 = []\n",
    "text_corpus_p2 = []\n",
    "text_corpus_p3 = []\n",
    "\n",
    "\n",
    "# Call the pre-processing function \n",
    "for line in text_corpus_p0:\n",
    "    # Ensure line is a string\n",
    "    if isinstance(line, list):\n",
    "        line = ' '.join(line)  # Join list items into a single string if needed\n",
    "    text = gensim.utils.simple_preprocess(line, deacc=True, min_len=6, max_len=50)\n",
    "    text_corpus_p1.append(text)\n",
    "\n",
    "# Create list of stopwords\n",
    "stoplist = set('for a of the and to in <email> <url> <phonenumber> <company>'.split(' '))\n",
    "\n",
    "# Filter out stopwords\n",
    "text_corpus_p2 = [[word for word in doc if word not in stoplist] for doc in text_corpus_p1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code removes all words that appear only once - does it make sense? Check with predicition results.\n",
    "# Count word frequencies\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in text_corpus_p2:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "text_corpus_p3 = [[token for token in text if frequency[token] > 3] for text in text_corpus_p2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Processed corpus\n",
    "text_corpus_p3 = []\n",
    "\n",
    "# Process each line\n",
    "for sublist in text_corpus_p2:\n",
    "    line = ' '.join(sublist)  # Join list elements into a single string\n",
    "    doc = nlp(line)  # Process the line with SpaCy\n",
    "    tokens = [token.text for token in doc]  # Extract tokens\n",
    "    lemmas = [token.lemma_ for token in doc]  # Extract lemmas\n",
    "    \n",
    "    text_corpus_p3.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the processed corpus\n",
    "for doc in text_corpus_p3:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with unique ids\n",
    "dictionary = corpora.Dictionary(text_corpus_p3)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test a phrase and generate a vector\n",
    "\n",
    "# new_doc = \"excellent student physics university student bananas\"\n",
    "# new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "# print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in text_corpus_p3]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train model to convert bow to vectors\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# # train the model\n",
    "# tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# # transform the \"system minors\" string\n",
    "# words = \"physics student university\".lower().split()\n",
    "# print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate a similarity index\n",
    "\n",
    "# from gensim import similarities\n",
    "\n",
    "# index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=13373)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query the similarity of a document to given string\n",
    "# query_document = 'system engineering'.split()\n",
    "# query_bow = dictionary.doc2bow(query_document)\n",
    "# sims = index[tfidf[query_bow]]\n",
    "# print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # same as above, but ordered\n",
    "\n",
    "# for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "#     print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora and Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('all_applicants.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate vectors with tf-idf for every line in the corpus\n",
    "# vector_corpus_tfidf = []\n",
    "\n",
    "# for line in text_corpus_p3:\n",
    "#     vector_corpus_tfidf.append(tfidf[dictionary.doc2bow(line)])\n",
    "\n",
    "#     # corpus = [dictionary.doc2bow(text) for text in texts] # shorter, without tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the processed corpus\n",
    "# for vec in vector_corpus_tfidf:\n",
    "#     print(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above works in the memory. If you have many documents, you can process them one by one.\n",
    "\n",
    "from smart_open import open  # for transparently opening remote files\n",
    "\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('https://radimrehurek.com/mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "\n",
    "The full power of Gensim comes from the fact that a corpus doesnâ€™t have to be a list, or a NumPy array, or a Pandas dataframe, or whatever. Gensim accepts any object that, when iterated over, successively yields documents.\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the lda model\n",
    "# 1min pro 1000 topics\n",
    "lda = models.LdaModel(bow_corpus, id2word=dictionary, num_topics=6500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model\n",
    "lda.save('lda_model_6500topics.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "# lda = models.LdaModel.load('lda_model_2500topics.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the corpus with lda probabilities\n",
    "# 70 seconds for 1000 topics\n",
    "\n",
    "vector_corpus_lda = []\n",
    "\n",
    "for line in text_corpus_p3:\n",
    "    vector_corpus_lda.append(lda[dictionary.doc2bow(line)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the processed corpus\n",
    "for vec in vector_corpus_lda:\n",
    "    print(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Sparse to Dense Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = lda.num_topics\n",
    "print(num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_dense(sparse_vec, num_topics):\n",
    "    dense_vec = np.zeros(num_topics)\n",
    "    for topic_id, topic_prob in sparse_vec:\n",
    "        dense_vec[topic_id] = topic_prob\n",
    "    return dense_vec\n",
    "\n",
    "dense_vectors = np.array([sparse_to_dense(doc, num_topics) for doc in vector_corpus_lda])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('agg-cv-labels.csv', header=None)\n",
    "labels = df.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dense_vectors, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],  # Example additional parameter\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best CV Accuracy: {best_score}')\n",
    "\n",
    "\n",
    "# Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
    "# Best Parameters: {'max_depth': 10, 'n_estimators': 300}\n",
    "# Best CV Accuracy: 0.6451469848421881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale the features\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# dense_vectors = scaler.fit_transform(dense_vectors)\n",
    "\n",
    "# # Train-Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dense_vectors, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define SVM model and hyperparameter grid\n",
    "# svm = SVC()\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1], # 10,100\n",
    "#     'gamma': [1, 0.1, 0.01, 0.001],\n",
    "#     'kernel': ['rbf', 'linear']\n",
    "# }\n",
    "\n",
    "# # Perform Grid Search\n",
    "# grid = GridSearchCV(svm, param_grid, refit=True, verbose=2, cv=5)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # Make Predictions and Evaluate\n",
    "# y_pred = grid.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Best Parameters: {grid.best_params_}')\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# # Best Parameters: {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
    "# # Accuracy: 0.6427480916030535\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import nbformat\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "randomseed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=100, svd_solver='randomized', random_state=randomseed) # Initialize with n_components parameter to only find the top eigenvectors\n",
    "vector_corpus_lda_pca = pca1.fit_transform(dense_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,n_init=100, random_state=randomseed, init='k-means++')\n",
    "kmeanclusters = kmeans.fit_predict(vector_corpus_lda_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vector_corpus_lda_pca[:,0:3], columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "fig = px.scatter_3d(df, x='PC1', y='PC2', z='PC3', title='3D Visualization of Top 3 Principal Components',   \n",
    "                    width=1000,\n",
    "                    height=800,\n",
    "                    color = kmeanclusters\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector_corpus_lda_pca, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model_LR = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_LR.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model_LR.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with selected features\n",
    "log_reg_c = LogisticRegressionCV(cv=5,Cs=[0.001,0.01,0.1,1,10],max_iter=5000,penalty=\"l2\",solver=\"liblinear\",multi_class=\"ovr\")\n",
    "log_reg_c.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result train\n",
    "print(log_reg_c.score(X_train, y_train))\n",
    "\n",
    "# result test\n",
    "print(log_reg_c.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector_corpus_lda_pca, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost classifier with default parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # For binary classification\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataVisualization_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
